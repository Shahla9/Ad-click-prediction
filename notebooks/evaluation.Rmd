---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.16.1
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

# init


## imports

```{python}
import matplotlib.pyplot as plt
import os
from sklearn.metrics import roc_auc_score, roc_curve
import warnings

warnings.filterwarnings('ignore')
```

## configs

```{python}
PREDICTION_DATA_PATH = os.path.expanduser('~/class/ad_click/data/test.csv')
```

# load data

```{python}
pred_df = pd.read_csv(PREDICTION_DATA_PATH)
pred_df
```

# evaluation


### auc curve

```{python}
results_dict = {}
auc_scores = {}
table_data = []
```

```{python}
true_labels = pred_df['clk']
xgs = 4

plt.figure(figsize=(4, 3))  # Adjust the figure size if needed

for xg in range(1, xgs):
    key = f'predicted_probabilities_{xg}'
    results_dict[key] = pred_df[f'Pred_xg{xg}']
    auc_score = roc_auc_score(true_labels, results_dict[key])
    auc_scores[xg] = auc_score
    table_data.append([f'Model {xg}', auc_score])
    fpr, tpr, _ = roc_curve(true_labels, results_dict[key])
    plt.plot(fpr, tpr, lw=2, label=f'Model {xg} (AUC = {auc_score:.1f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.show()
```
