---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.16.1
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

# init


## imports

```{python}
import math
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
from sklearn.metrics import roc_auc_score, roc_curve
from sklearn.metrics import precision_recall_curve, average_precision_score
import warnings

from tabulate import tabulate

warnings.filterwarnings('ignore')
```

## configs

```{python}
PREDICTION_DATA_PATH = ('/home/shaghayegh/class/ad_click/data/test.csv')
```

# load data

```{python}
pred_df = pd.read_csv(PREDICTION_DATA_PATH)
pred_df
```

# evaluation


## methods

```{python}
results_dict = {}
auc_scores = {}
```

### auc curve

```{python}
true_labels = pred_df['clk']

xgs = 4

for xg in range(1, xgs):
    key = f'predicted_probabilities_{xg}'
    results_dict[key] = pred_df[f'Pred_xg{xg}']
    auc_scores[xg] = roc_auc_score(true_labels, results_dict[key])
    fpr, tpr, _ = roc_curve(true_labels, results_dict[key])

plt.figure(figsize=(4, 3))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {auc_score:.2f}')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.show()
```

## summerize

```{python}
result_table = [
    [method, results_dict[method]['mape'], results_dict[method]['rmse']]
    for method in results_dict
]
print(tabulate(result_table, headers=["Method","MAPE", "RMSE"], tablefmt="simple_grid"))
```
